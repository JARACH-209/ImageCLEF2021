{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu_devices[0], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ViT_model_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser('~')\n",
    "base = os.path.join('Datasets', 'ImageCLEF', 'Mean_Slice_Masks')\n",
    "\n",
    "train_dir = os.path.join(home, base, 'train')\n",
    "test_dir = os.path.join(home, base, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 731 images belonging to 5 classes.\n",
      "Found 184 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "shuffle = True\n",
    "inp_shp = (380, 380)\n",
    "train_batch_size, val_batch_size = 8, 64\n",
    "\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=inp_shp,\n",
    "        batch_size=train_batch_size,\n",
    "        seed=seed,\n",
    "        class_mode='categorical',\n",
    "        color_mode='rgb',\n",
    "        shuffle=shuffle\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=inp_shp,\n",
    "        batch_size=val_batch_size,\n",
    "        seed=seed,\n",
    "        class_mode='categorical',\n",
    "        color_mode='rgb',\n",
    "        shuffle=shuffle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "input_shape = (380, 380, 3)\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 8\n",
    "num_epochs = 100\n",
    "image_size = 380  # We'll resize input images to this size\n",
    "# Total Patches/image with 19 patch_size = 400\n",
    "patch_size = 19  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 254\n",
    "num_heads = 16 #number of heads used for attention\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 24\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 380 X 380\n",
      "Patch size: 19 X 19\n",
      "Patches per image: 400\n",
      "Elements per patch: 1083\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAAATCAYAAAB81YrmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAaElEQVR4nO3aMQqAQAwAQSP+/8vxAyJYycJMedekWVLcze4eQMf59wDAN6KFGNFCjGghRrQQI1qIud4uZ8Z7EPxkd+fp3KaFGNFCjGghRrQQM/4eQ4tNCzGihRjRQoxoIUa0ECNaiLkB0IsKIUnxLgsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADnCAYAAADy1tHpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD7klEQVR4nO3bMQobSxBF0Sfj/W9ZPzRMYNqB5/c156QqRCWXAtH6fL/fAR0//u8FgD8jWogRLcSIFmJECzE/f/fh5/M5+mn5+/1+/mT2b363nX/Nbnb+F/Z4cmkhRrQQI1qIES3EiBZiRAsxooUY0ULMx1/zoMWlhRjPGA9nNzu/MbvdsfNNezy5tBAjWogRLcSIFmJECzGihRjRQoxoIcYzRohxaSHGM8bD2c3Ob8xud+x80x5PLi3EiBZiRAsxooUY0UKMaCFGtBAjWojxjBFiXFqI8YzxcHaz8xuz2x0737THk0sLMaKFGNFCjGghRrQQI1qIES3EiBZiPGOEGJcWYjxjPJzd7PzG7HbHzjft8eTSQoxoIUa0ECNaiBEtxIgWYkQLMaKFGM8YIcalhRjPGA9nNzu/MbvdsfNNezy5tBAjWogRLcSIFmJECzGihRjRQoxoIcYzRohxaSHGM8bD2c3Ob8xud+x80x5PLi3EiBZiRAsxooUY0UKMaCFGtBAjWojxjBFiXFqI8YzxcHaz8xuz2x0737THk0sLMaKFGNFCjGghRrQQI1qIES3EiBZiPGOEGJcWYjxjPJzd7PzG7HbHzjft8eTSQoxoIUa0ECNaiBEtxIgWYkQLMaKFGM8YIcalhRjPGA9nNzu/MbvdsfNNezy5tBAjWogRLcSIFmJECzGihRjRQoxoIcYzRohxaSHGM8bD2c3Ob8xud+x80x5PLi3EiBZiRAsxooUY0UKMaCFGtBAjWojxjBFiXFqI8YzxcHaz8xuz2x0737THk0sLMaKFGNFCjGghRrQQI1qIES3EiBZiPGOEGJcWYjxjPJzd7PzG7HbHzjft8eTSQoxoIUa0ECNaiBEtxIgWYkQLMaKFGM8YIcalhRjPGA9nNzu/MbvdsfNNezy5tBAjWogRLcSIFmJECzGihRjRQoxoIcYzRohxaSHGM8bD2c3Ob8xud+x80x5PLi3EiBZiRAsxooUY0UKMaCFGtBAjWojxjBFiXFqI8YzxcHaz8xuz2x0737THk0sLMaKFGNFCjGghRrQQI1qIES3EiBZiPGOEGJcWYjxjPJzd7PzG7HbHzjft8eTSQoxoIUa0ECNaiBEtxIgWYkQLMaKFGM8YIcalhRjPGA9nNzu/MbvdsfNNezy5tBAjWogRLcSIFmJECzGihRjRQoxoIcYzRohxaSHGM8bD2c3Ob8xud+x80x5PLi3EiBZiRAsxooUY0UKMaCFGtBAjWojxjBFiXFqI8YzxcHaz8xuz2x0737THk0sLMaKFGNFCjGghRrQQI1qIES3EiBZiPGOEGJcWYkQLMaKFGNFCjGghRrQQ8x8/x4D9zzUjRwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 400 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "batch = next(train_generator)\n",
    "image = batch[0][:,:,1]\n",
    "plt.imshow(np.asarray(image))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'einsum_dense' from 'tensorflow.python.keras.layers' (/home/ayushman.singh/anaconda3/envs/tflow/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-75162d346271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meinsum_dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'einsum_dense' from 'tensorflow.python.keras.layers' (/home/ayushman.singh/anaconda3/envs/tflow/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.engine.base_layer import Layer\n",
    "import collections\n",
    "import math\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras import constraints\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.engine.base_layer import Layer\n",
    "from tensorflow.python.keras.layers import advanced_activations\n",
    "from tensorflow.python.keras.layers import core\n",
    "from tensorflow.python.keras.layers import einsum_dense\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import special_math_ops\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "\n",
    "def _build_attention_equation(rank, attn_axes):\n",
    "  \"\"\"Builds einsum equations for the attention computation.\n",
    "  Query, key, value inputs after projection are expected to have the shape as:\n",
    "  (bs, <non-attention dims>, <attention dims>, num_heads, channels).\n",
    "  bs and <non-attention dims> are treated as <batch dims>.\n",
    "  The attention operations can be generalized:\n",
    "  (1) Query-key dot product:\n",
    "  (<batch dims>, <query attention dims>, num_heads, channels), (<batch dims>,\n",
    "  <key attention dims>, num_heads, channels) -> (<batch dims>,\n",
    "  num_heads, <query attention dims>, <key attention dims>)\n",
    "  (2) Combination:\n",
    "  (<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n",
    "  (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch dims>,\n",
    "  <query attention dims>, num_heads, channels)\n",
    "  Args:\n",
    "    rank: the rank of query, key, value tensors.\n",
    "    attn_axes: a list/tuple of axes, [-1, rank), that will do attention.\n",
    "  Returns:\n",
    "    Einsum equations.\n",
    "  \"\"\"\n",
    "  target_notation = _CHR_IDX[:rank]\n",
    "  # `batch_dims` includes the head dim.\n",
    "  batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n",
    "  letter_offset = rank\n",
    "  source_notation = \"\"\n",
    "  for i in range(rank):\n",
    "    if i in batch_dims or i == rank - 1:\n",
    "      source_notation += target_notation[i]\n",
    "    else:\n",
    "      source_notation += _CHR_IDX[letter_offset]\n",
    "      letter_offset += 1\n",
    "\n",
    "  product_notation = \"\".join([target_notation[i] for i in batch_dims] +\n",
    "                             [target_notation[i] for i in attn_axes] +\n",
    "                             [source_notation[i] for i in attn_axes])\n",
    "  dot_product_equation = \"%s,%s->%s\" % (source_notation, target_notation,\n",
    "                                        product_notation)\n",
    "  attn_scores_rank = len(product_notation)\n",
    "  combine_equation = \"%s,%s->%s\" % (product_notation, source_notation,\n",
    "                                    target_notation)\n",
    "  return dot_product_equation, combine_equation, attn_scores_rank\n",
    "\n",
    "\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "  \"\"\"Builds an einsum equation for projections inside multi-head attention.\"\"\"\n",
    "  input_str = \"\"\n",
    "  kernel_str = \"\"\n",
    "  output_str = \"\"\n",
    "  bias_axes = \"\"\n",
    "  letter_offset = 0\n",
    "  for i in range(free_dims):\n",
    "    char = _CHR_IDX[i + letter_offset]\n",
    "    input_str += char\n",
    "    output_str += char\n",
    "\n",
    "  letter_offset += free_dims\n",
    "  for i in range(bound_dims):\n",
    "    char = _CHR_IDX[i + letter_offset]\n",
    "    input_str += char\n",
    "    kernel_str += char\n",
    "\n",
    "  letter_offset += bound_dims\n",
    "  for i in range(output_dims):\n",
    "    char = _CHR_IDX[i + letter_offset]\n",
    "    kernel_str += char\n",
    "    output_str += char\n",
    "    bias_axes += char\n",
    "  equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "  return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "  return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "  \"\"\"MultiHeadAttention layer.\n",
    "  This is an implementation of multi-headed attention based on \"Attention\n",
    "  is all you Need\". If `query`, `key,` `value` are the same, then\n",
    "  this is self-attention. Each timestep in `query` attends to the\n",
    "  corresponding sequence in `key`, and returns a fixed-width vector.\n",
    "  This layer first projects `query`, `key` and `value`. These are\n",
    "  (effectively) a list of tensors of length `num_attention_heads`, where the\n",
    "  corresponding shapes are [batch_size, <query dimensions>, key_dim],\n",
    "  [batch_size, <key/value dimensions>, key_dim],\n",
    "  [batch_size, <key/value dimensions>, value_dim].\n",
    "  Then, the query and key tensors are dot-producted and scaled. These are\n",
    "  softmaxed to obtain attention probabilities. The value tensors are then\n",
    "  interpolated by these probabilities, then concatenated back to a single\n",
    "  tensor.\n",
    "  Finally, the result tensor with the last dimension as value_dim can take an\n",
    "  linear projection and return.\n",
    "  Examples:\n",
    "  Performs 1D cross-attention over two sequence inputs with an attention mask.\n",
    "  Returns the additional attention weights over heads.\n",
    "  >>> layer = MultiHeadAttention(num_heads=2, key_dim=2)\n",
    "  >>> target = tf.keras.Input(shape=[8, 16])\n",
    "  >>> source = tf.keras.Input(shape=[4, 16])\n",
    "  >>> output_tensor, weights = layer(target, source,\n",
    "  ...                                return_attention_scores=True)\n",
    "  >>> print(output_tensor.shape)\n",
    "  (None, 8, 16)\n",
    "  >>> print(weights.shape)\n",
    "  (None, 2, 8, 4)\n",
    "  Performs 2D self-attention over a 5D input tensor on axes 2 and 3.\n",
    "  >>> layer = MultiHeadAttention(num_heads=2, key_dim=2, attention_axes=(2, 3))\n",
    "  >>> input_tensor = tf.keras.Input(shape=[5, 3, 4, 16])\n",
    "  >>> output_tensor = layer(input_tensor, input_tensor)\n",
    "  >>> print(output_tensor.shape)\n",
    "  (None, 5, 3, 4, 16)\n",
    "  Arguments:\n",
    "    num_heads: Number of attention heads.\n",
    "    key_dim: Size of each attention head for query and key.\n",
    "    value_dim:  Size of each attention head for value.\n",
    "    dropout: Dropout probability.\n",
    "    use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n",
    "    output_shape: The expected shape of an output tensor, besides the batch and\n",
    "      sequence dims. If not specified, projects back to the key feature dim.\n",
    "    attention_axes: axes over which the attention is applied. `None` means\n",
    "      attention over all axes, but batch, heads, and features.\n",
    "    kernel_initializer: Initializer for dense layer kernels.\n",
    "    bias_initializer: Initializer for dense layer biases.\n",
    "    kernel_regularizer: Regularizer for dense layer kernels.\n",
    "    bias_regularizer: Regularizer for dense layer biases.\n",
    "    activity_regularizer: Regularizer for dense layer activity.\n",
    "    kernel_constraint: Constraint for dense layer kernels.\n",
    "    bias_constraint: Constraint for dense layer kernels.\n",
    "  Call arguments:\n",
    "    query: Query `Tensor` of shape `[B, T, dim]`.\n",
    "    value: Value `Tensor` of shape `[B, S, dim]`.\n",
    "    key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n",
    "      `value` for both `key` and `value`, which is the most common case.\n",
    "    attention_mask: a boolean mask of shape `[B, T, S]`, that prevents\n",
    "      attention to certain positions.\n",
    "    return_attention_scores: A boolean to indicate whether the output should\n",
    "      be attention output if True, or (attention_output, attention_scores) if\n",
    "      False. Defaults to False.\n",
    "    training: Python boolean indicating whether the layer should behave in\n",
    "      training mode (adding dropout) or in inference mode (no dropout).\n",
    "      Defaults to either using the training mode of the parent layer/model,\n",
    "      or False (inference) if there is no parent layer.\n",
    "  Returns:\n",
    "    attention_output: The result of the computation, of shape [B, T, E],\n",
    "      where `T` is for target sequence shapes and `E` is the query input last\n",
    "      dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "      are project to the shape specified by `output_shape`.\n",
    "    attention_scores: [Optional] multi-head attention coeffients over\n",
    "      attention axes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_heads,\n",
    "               key_dim,\n",
    "               value_dim=None,\n",
    "               dropout=0.0,\n",
    "               use_bias=True,\n",
    "               output_shape=None,\n",
    "               attention_axes=None,\n",
    "               kernel_initializer=\"glorot_uniform\",\n",
    "               bias_initializer=\"zeros\",\n",
    "               kernel_regularizer=None,\n",
    "               bias_regularizer=None,\n",
    "               activity_regularizer=None,\n",
    "               kernel_constraint=None,\n",
    "               bias_constraint=None,\n",
    "               **kwargs):\n",
    "    super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "    self._num_heads = num_heads\n",
    "    self._key_dim = key_dim\n",
    "    self._value_dim = value_dim if value_dim else key_dim\n",
    "    self._dropout = dropout\n",
    "    self._use_bias = use_bias\n",
    "    self._output_shape = output_shape\n",
    "    self._kernel_initializer = initializers.get(kernel_initializer)\n",
    "    self._bias_initializer = initializers.get(bias_initializer)\n",
    "    self._kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "    self._bias_regularizer = regularizers.get(bias_regularizer)\n",
    "    self._kernel_constraint = constraints.get(kernel_constraint)\n",
    "    self._bias_constraint = constraints.get(bias_constraint)\n",
    "    if attention_axes is not None and not isinstance(attention_axes,\n",
    "                                                     collections.abc.Sized):\n",
    "      self._attention_axes = (attention_axes,)\n",
    "    else:\n",
    "      self._attention_axes = attention_axes\n",
    "    self._built_from_signature = False\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        \"num_heads\":\n",
    "            self._num_heads,\n",
    "        \"key_dim\":\n",
    "            self._key_dim,\n",
    "        \"value_dim\":\n",
    "            self._value_dim,\n",
    "        \"dropout\":\n",
    "            self._dropout,\n",
    "        \"use_bias\":\n",
    "            self._use_bias,\n",
    "        \"output_shape\":\n",
    "            self._output_shape,\n",
    "        \"attention_axes\":\n",
    "            self._attention_axes,\n",
    "        \"kernel_initializer\":\n",
    "            initializers.serialize(self._kernel_initializer),\n",
    "        \"bias_initializer\":\n",
    "            initializers.serialize(self._bias_initializer),\n",
    "        \"kernel_regularizer\":\n",
    "            regularizers.serialize(self._kernel_regularizer),\n",
    "        \"bias_regularizer\":\n",
    "            regularizers.serialize(self._bias_regularizer),\n",
    "        \"activity_regularizer\":\n",
    "            regularizers.serialize(self._activity_regularizer),\n",
    "        \"kernel_constraint\":\n",
    "            constraints.serialize(self._kernel_constraint),\n",
    "        \"bias_constraint\":\n",
    "            constraints.serialize(self._bias_constraint)\n",
    "    }\n",
    "    base_config = super(MultiHeadAttention, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "  def _build_from_signature(self, query, value, key=None):\n",
    "    \"\"\"Builds layers and variables.\n",
    "    Once the method is called, self._built_from_signature will be set to True.\n",
    "    Args:\n",
    "      query: query tensor or TensorShape.\n",
    "      value: value tensor or TensorShape.\n",
    "      key: key tensor or TensorShape.\n",
    "    \"\"\"\n",
    "    self._built_from_signature = True\n",
    "    if hasattr(query, \"shape\"):\n",
    "      query_shape = tensor_shape.TensorShape(query.shape)\n",
    "    else:\n",
    "      query_shape = query\n",
    "    if hasattr(value, \"shape\"):\n",
    "      value_shape = tensor_shape.TensorShape(value.shape)\n",
    "    else:\n",
    "      value_shape = value\n",
    "    if key is None:\n",
    "      key_shape = value_shape\n",
    "    elif hasattr(key, \"shape\"):\n",
    "      key_shape = tensor_shape.TensorShape(key.shape)\n",
    "    else:\n",
    "      key_shape = key\n",
    "\n",
    "    common_kwargs = dict(\n",
    "        kernel_initializer=self._kernel_initializer,\n",
    "        bias_initializer=self._bias_initializer,\n",
    "        kernel_regularizer=self._kernel_regularizer,\n",
    "        bias_regularizer=self._bias_regularizer,\n",
    "        activity_regularizer=self._activity_regularizer,\n",
    "        kernel_constraint=self._kernel_constraint,\n",
    "        bias_constraint=self._bias_constraint)\n",
    "    # Any setup work performed only once should happen in an `init_scope`\n",
    "    # to avoid creating symbolic Tensors that will later pollute any eager\n",
    "    # operations.\n",
    "    with tf_utils.maybe_init_scope(self):\n",
    "      free_dims = query_shape.rank - 1\n",
    "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "          free_dims, bound_dims=1, output_dims=2)\n",
    "      self._query_dense = einsum_dense.EinsumDense(\n",
    "          einsum_equation,\n",
    "          output_shape=_get_output_shape(output_rank - 1,\n",
    "                                         [self._num_heads, self._key_dim]),\n",
    "          bias_axes=bias_axes if self._use_bias else None,\n",
    "          name=\"query\",\n",
    "          **common_kwargs)\n",
    "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "          key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "      self._key_dense = einsum_dense.EinsumDense(\n",
    "          einsum_equation,\n",
    "          output_shape=_get_output_shape(output_rank - 1,\n",
    "                                         [self._num_heads, self._key_dim]),\n",
    "          bias_axes=bias_axes if self._use_bias else None,\n",
    "          name=\"key\",\n",
    "          **common_kwargs)\n",
    "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "          value_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "      self._value_dense = einsum_dense.EinsumDense(\n",
    "          einsum_equation,\n",
    "          output_shape=_get_output_shape(output_rank - 1,\n",
    "                                         [self._num_heads, self._value_dim]),\n",
    "          bias_axes=bias_axes if self._use_bias else None,\n",
    "          name=\"value\",\n",
    "          **common_kwargs)\n",
    "\n",
    "      # Builds the attention computations for multi-head dot product attention.\n",
    "      # These computations could be wrapped into the keras attention layer once\n",
    "      # it support mult-head einsum computations.\n",
    "      self._build_attention(output_rank)\n",
    "      if self._output_shape:\n",
    "        if not isinstance(self._output_shape, collections.abc.Sized):\n",
    "          output_shape = [self._output_shape]\n",
    "        else:\n",
    "          output_shape = self._output_shape\n",
    "      else:\n",
    "        output_shape = [query_shape[-1]]\n",
    "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "          free_dims, bound_dims=2, output_dims=len(output_shape))\n",
    "      self._output_dense = einsum_dense.EinsumDense(\n",
    "          einsum_equation,\n",
    "          output_shape=_get_output_shape(output_rank - 1, output_shape),\n",
    "          bias_axes=bias_axes if self._use_bias else None,\n",
    "          name=\"attention_output\",\n",
    "          **common_kwargs)\n",
    "\n",
    "  def _build_attention(self, rank):\n",
    "    \"\"\"Builds multi-head dot-product attention computations.\n",
    "    This function builds attributes necessary for `_compute_attention` to\n",
    "    costomize attention computation to replace the default dot-product\n",
    "    attention.\n",
    "    Args:\n",
    "      rank: the rank of query, key, value tensors.\n",
    "    \"\"\"\n",
    "    if self._attention_axes is None:\n",
    "      self._attention_axes = tuple(range(1, rank - 2))\n",
    "    else:\n",
    "      self._attention_axes = tuple(self._attention_axes)\n",
    "    self._dot_product_equation, self._combine_equation, attn_scores_rank = (\n",
    "        _build_attention_equation(rank, attn_axes=self._attention_axes))\n",
    "    norm_axes = tuple(\n",
    "        range(attn_scores_rank - len(self._attention_axes), attn_scores_rank))\n",
    "    self._softmax = advanced_activations.Softmax(axis=norm_axes)\n",
    "    self._dropout_layer = core.Dropout(rate=self._dropout)\n",
    "\n",
    "  def _masked_softmax(self, attention_scores, attention_mask=None):\n",
    "    # Normalize the attention scores to probabilities.\n",
    "    # `attention_scores` = [B, N, T, S]\n",
    "    if attention_mask is not None:\n",
    "      # The expand dim happens starting from the `num_heads` dimension,\n",
    "      # (<batch_dims>, num_heads, <query_attention_dims, key_attention_dims>)\n",
    "      mask_expansion_axes = [-len(self._attention_axes) * 2 - 1]\n",
    "      for _ in range(len(attention_scores.shape) - len(attention_mask.shape)):\n",
    "        attention_mask = array_ops.expand_dims(\n",
    "            attention_mask, axis=mask_expansion_axes)\n",
    "    return self._softmax(attention_scores, attention_mask)\n",
    "\n",
    "  def _compute_attention(self,\n",
    "                         query,\n",
    "                         key,\n",
    "                         value,\n",
    "                         attention_mask=None,\n",
    "                         training=None):\n",
    "    \"\"\"Applies Dot-product attention with query, key, value tensors.\n",
    "    This function defines the computation inside `call` with projected\n",
    "    multi-head Q, K, V inputs. Users can override this function for customized\n",
    "    attention implementation.\n",
    "    Args:\n",
    "      query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "      key: Projected key `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "      value: Projected value `Tensor` of shape `[B, T, N, value_dim]`.\n",
    "      attention_mask: a boolean mask of shape `[B, T, S]`, that prevents\n",
    "        attention to certain positions.\n",
    "      training: Python boolean indicating whether the layer should behave in\n",
    "        training mode (adding dropout) or in inference mode (doing nothing).\n",
    "    Returns:\n",
    "      attention_output: Multi-headed outputs of attention computation.\n",
    "      attention_scores: Multi-headed attention weights.\n",
    "    \"\"\"\n",
    "    # Note: Applying scalar multiply at the smaller end of einsum improves\n",
    "    # XLA performance, but may introduce slight numeric differences in\n",
    "    # the Transformer attention head.\n",
    "    query = math_ops.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "    # Take the dot product between \"query\" and \"key\" to get the raw\n",
    "    # attention scores.\n",
    "    attention_scores = special_math_ops.einsum(self._dot_product_equation, key,\n",
    "                                               query)\n",
    "\n",
    "    attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "    # This is actually dropping out entire tokens to attend to, which might\n",
    "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "    attention_scores_dropout = self._dropout_layer(\n",
    "        attention_scores, training=training)\n",
    "\n",
    "    # `context_layer` = [B, T, N, H]\n",
    "    attention_output = special_math_ops.einsum(self._combine_equation,\n",
    "                                               attention_scores_dropout, value)\n",
    "    return attention_output, attention_scores\n",
    "\n",
    "  def call(self,\n",
    "           query,\n",
    "           value,\n",
    "           key=None,\n",
    "           attention_mask=None,\n",
    "           return_attention_scores=False,\n",
    "           training=None):\n",
    "    if not self._built_from_signature:\n",
    "      self._build_from_signature(query=query, value=value, key=key)\n",
    "    if key is None:\n",
    "      key = value\n",
    "\n",
    "    #   N = `num_attention_heads`\n",
    "    #   H = `size_per_head`\n",
    "    # `query` = [B, T, N ,H]\n",
    "    query = self._query_dense(query)\n",
    "\n",
    "    # `key` = [B, S, N, H]\n",
    "    key = self._key_dense(key)\n",
    "\n",
    "    # `value` = [B, S, N, H]\n",
    "    value = self._value_dense(value)\n",
    "\n",
    "    attention_output, attention_scores = self._compute_attention(\n",
    "        query, key, value, attention_mask, training)\n",
    "    attention_output = self._output_dense(attention_output)\n",
    "\n",
    "    if return_attention_scores:\n",
    "      return attention_output, attention_scores\n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "#     augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initializers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-c03628317ba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mvit_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vit_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-bcea2e53dee4>\u001b[0m in \u001b[0;36mcreate_vit_classifier\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Create a multi-head attention layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         attention_output = MultiHeadAttention(\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojection_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         )(x1, x1)\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Skip connection 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-e8e15914daea>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_heads, key_dim, value_dim, dropout, use_bias, output_shape, attention_axes, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bias_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kernel_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initializers' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow_addons.losses import SigmoidFocalCrossEntropy\n",
    "from tensorflow_addons.metrics import CohenKappa\n",
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=SigmoidalFocalCrossEntropy(),\n",
    "        metrics=['acc',CohenKappa(num_classes=5, sparse_labels=True)],\n",
    "    )\n",
    "\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    f\"{model_name}.h5\", save_best_only=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-c178374322e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mearly_stopping_cb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    f\"{model_name}.h5\", save_best_only=True\n",
    ")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=64)\n",
    "\n",
    "history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=64,\n",
    "            epochs=256,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=1,\n",
    "            shuffle=False,\n",
    "            callbacks=[checkpoint_cb, early_stopping_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f52380bab10>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{model_name}_history.pkl', 'wb') as fh:\n",
    "    pickle.dump(history.history, fh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
